acajou_digital:
  extraction_dest_relative_path: "acajou_digital/extracted/"
  transformation_dest_relative_path: "acajou_digital/transformed/"
  error_dest_relative_path: "acajou_digital/error/"
  loaded_dest_relative_path: "acajou_digital/loaded/"
  processed_dest_relative_path: "acajou_digital/processed/"
  file_downloaded_name: "Accounting.csv" # Nom du fichier tel qu'il est téléchargé depuis la plateforme
  file_pattern: "Accounting.csv" # Motif pour retrouver le fichier après téléchargement (peut inclure des wildcards si le nom varie)
  wait_time: 60 # Temps d'attente pour le téléchargement en secondes, augmenté pour plus de robustesse
  # download_path: "C:/ETL/DATA_FICHIERS/acajou_digital/extracted/" # Ce chemin sera maintenant géré par la config globale (DOWNLOAD_PATH) et la classe de base.
                                                                  # La classe de base construira {DOWNLOAD_PATH}/{job_name}/ pour les téléchargements bruts si nécessaire,
                                                                  # ou utilisera directement DOWNLOAD_PATH si les fichiers ne sont pas spécifiques à un job au moment du téléchargement.
                                                                  # Pour ce scraper, il semble que les fichiers soient directement téléchargés dans un dossier qui sera ensuite
                                                                  # extraction_dest_relative_path. La config de BaseScrapper utilisera 'download_path' du job_config
                                                                  # qui sera maintenant {DATA_PATH}/acajou_digital/extracted/
  download_path: "acajou_digital/extracted/" # Relatif à DATA_PATH, ou un placeholder qui sera résolu.
                                             # Pour BaseScrapper, il est préférable que ce soit un chemin absolu ou résolu.
                                             # La classe BaseScrapper utilisera `self.config["download_path"]` qui est résolu par `get_config`
                                             # pour pointer vers `DATA_PATH / extraction_dest_relative_path`.
                                             # Donc, cette clé "download_path" ici pourrait devenir redondante si extraction_dest_relative_path est utilisé comme LA destination de téléchargement.
                                             # Ou, si le téléchargement initial se fait dans un dossier "global" de téléchargement,
                                             # alors BaseScrapper utilisera le DOWNLOAD_PATH global et déplacera ensuite les fichiers.
                                             # Pour l'instant, on va supposer que le BaseScrapper est configuré pour télécharger
                                             # directement dans DATA_PATH/extraction_dest_relative_path.
                                             # Donc, cette clé "download_path" au niveau du job n'est plus nécessaire si BaseScrapper utilise DATA_PATH/extraction_dest_relative_path.
                                             # Cependant, BaseScrapper actuel utilise `self.config["download_path"]` pour les préférences Chrome.
                                             # Nous allons donc le définir pour qu'il pointe vers la destination d'extraction.

  ##### extraction config #####
  start_date:   # defaut is today - 1 day. Format YYYY-MM-DD si spécifié.
  end_date:
  include_sup: false
  urls:
    login: "https://cms.acajou.sn/Account/Login"
    report: "https://cms.acajou.sn/Accounting/AccountingReport"

  html_elements:
    ### login page ###
    username_element_id: "Input_Email"
    password_element_id: "Input_Password"
    login_submit_button_element_xpath: "/html/body/div/main/div/div[2]/div/form/div[3]/div[2]/button"
    verification_xpath: "/html/body/div/aside/div/nav/ul/li/a"

    ### report page ###
    date_element_name: "dateCreated"
    step_x1_element_id: "results-jackpot_processing"
    step_x2_element_xpath: "/html/body/div[1]/div[1]/section[2]/div/div/div/div/div[3]/button"

    date_day_element_id: "OpsValidator_ContentPlaceHolder1_FDate_ddlDate"
    date_month_element_id: "OpsValidator_ContentPlaceHolder1_FDate_ddlMonth"
    date_year_element_id: "OpsValidator_ContentPlaceHolder1_FDate_ddlYear"
    submit_button_element_id: "OpsValidator_ContentPlaceHolder1_btnSubmit"
    download_button_element_id: "OpsValidator_ContentPlaceHolder1_btnExcel"
    error_message_element_xpath: "//span[contains(@id,'OpsValidator_ContentPlaceHolder1_lblErrInfo') and contains(text(),'Aucun enregistrement trouv')]"

  #### transform config ####
  files_to_transform_pattern: "acajou_digital*csv"

  #### load config ####
  files_to_load_pattern: "acajou_digital_transformed*.csv"

